{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa02edc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 08:39:49.494081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/bhavanishankar/opt/anaconda3/envs/tensyflow/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import BatchNormalization, Bidirectional, LSTM, concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pickle\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c063582",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='./Data/xnli_hi_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0851423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(filepath):\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data = data[list(data.keys())[0]]\n",
    "        #print(data)\n",
    "    for idx, row in enumerate(data):\n",
    "        premise.append(row[\"premise\"])\n",
    "        hypothesis.append(row[\"hypothesis\"])\n",
    "        label.append(row[\"label\"])\n",
    "    df = pd.DataFrame(list(zip(premise, hypothesis,label)),\n",
    "               columns =['premise', 'hypothesis','label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9ef80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf012446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अवधारणात्मक रूप से क्रीम स्किमिंग के दो बुनिया...</td>\n",
       "      <td>उत्पाद और भूगोल क्रीम स्किमिंग का काम करते हैं।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>आप मौसम के दौरान पता है और मुझे लगता है कि अपन...</td>\n",
       "      <td>अगर लोग याद करते हैं तो आप निम्नलिखित स्तर पर ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>हमारी संख्या में से एक आपके निर्देशों का बारीक...</td>\n",
       "      <td>मेरी टीम का एक सदस्य आपके आदेशों को बहुत सटीकत...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>तुम्हें कैसे पता? यह सब फिर से उनकी जानकारी है.</td>\n",
       "      <td>यह जानकारी उनके पास है।</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>हाँ मैं आपको बताती हूँ कि अगर आप उन टेनिस जूतो...</td>\n",
       "      <td>टेनिस जूतों की कीमतों की एक श्रृंखला है।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  अवधारणात्मक रूप से क्रीम स्किमिंग के दो बुनिया...   \n",
       "1  आप मौसम के दौरान पता है और मुझे लगता है कि अपन...   \n",
       "2  हमारी संख्या में से एक आपके निर्देशों का बारीक...   \n",
       "3    तुम्हें कैसे पता? यह सब फिर से उनकी जानकारी है.   \n",
       "4  हाँ मैं आपको बताती हूँ कि अगर आप उन टेनिस जूतो...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0    उत्पाद और भूगोल क्रीम स्किमिंग का काम करते हैं।      1  \n",
       "1  अगर लोग याद करते हैं तो आप निम्नलिखित स्तर पर ...      0  \n",
       "2  मेरी टीम का एक सदस्य आपके आदेशों को बहुत सटीकत...      0  \n",
       "3                            यह जानकारी उनके पास है।      0  \n",
       "4           टेनिस जूतों की कीमतों की एक श्रृंखला है।      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d84f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # Get the sentences and labels from composite data\n",
    "    list_premise = data['premise']\n",
    "    list_hypothesis = data['hypothesis']\n",
    "    list_label = data['label']\n",
    "    # Merge each sublist (tokens list of each sentence) to a string\n",
    "    corpus_premise = [''.join(item) for item in list_premise]\n",
    "    corpus_hypothesis = [''.join(item) for item in list_hypothesis]\n",
    "    num_samples = len(list_label)\n",
    "    labels = np.array(list_label)\n",
    "    corpus = [corpus_premise[ind] + \" \" + corpus_hypothesis[ind] for ind in range(len(labels))]\n",
    "    \n",
    "    return corpus_premise, corpus_hypothesis, labels, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1703a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_list, hypothesis_list, labels_list, corpus = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c931d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# # Save the tokenizer as a pickle file so that the same tokenizer (word-integer)\n",
    "# # mapping can be used during testing time\n",
    "# with open('./tokenizer.pickle', \"wb\") as file:\n",
    "#     pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0307bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 #FastText output dimensions for each wordvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "323a96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "ft = fasttext.load_model('Data/cc.hi.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f98c9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, ind in word_index.items():\n",
    "    # Get the embedding vector from FastText ouput, if available\n",
    "    embedding_vector = ft.get_word_vector(word)\n",
    "#     print(embedding_vector)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[ind] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3d9e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 281\n",
    "sequence = lambda sentence: pad_sequences(tokenizer.texts_to_sequences(sentence), maxlen=MAX_SEQ_LEN)\n",
    "process = lambda item: (sequence(item[0]), sequence(item[1]), to_categorical(item[2]))\n",
    "\n",
    "training_data = process([premise_list, hypothesis_list, labels_list])\n",
    "print(len(word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403a77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "VOCAB_SIZE = 81649\n",
    "EMBEDDING_HIDDEN_SIZE = 300\n",
    "SENT_HIDDEN_SIZE = 300\n",
    "TRAIN_EMBED = False\n",
    "\n",
    "L2 = 4e-6\n",
    "ACTIVATION = 'relu'\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.01\n",
    "RHO = 0.9\n",
    "EPSILON = 1e-08\n",
    "DECAY = 0.0\n",
    "\n",
    "CATEGORIES = 3\n",
    "BATCH_SIZE = 512\n",
    "TRAINING_EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.02\n",
    "\n",
    "PATIENCE = 4\n",
    "\n",
    "\n",
    "def BiLSTM(premise_list, hypothesis_list, labels_list, embedding_matrix):\n",
    "    #Embedding layer with obtained embedding matrix\n",
    "    # Define the embedding layer with the obtained weight matrix\n",
    "    embedding = Embedding(input_dim = embedding_matrix.shape[0], output_dim = EMBEDDING_HIDDEN_SIZE, weights = [embedding_matrix], input_length = MAX_SEQ_LEN, trainable = TRAIN_EMBED)\n",
    "    BiLSTM = Bidirectional(LSTM(LSTM_UNITS)) #BiLSTM Layer\n",
    "    translation = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION)) #Time Distributed Layer to increase performance\n",
    "    \n",
    "    # Defining the input layers and its shapes for premise and hypothesis\n",
    "    premise = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "    hypothesis = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "    # Embed the premise and hypothesis\n",
    "    premise_embedded = embedding(premise)\n",
    "    hypothesis_embedded = embedding(hypothesis)\n",
    "\n",
    "    # Apply the translation layer\n",
    "    premise_translated = translation(premise_embedded)\n",
    "    hypothesis_translated = translation(hypothesis_embedded)\n",
    "\n",
    "    # Apply the bidirectional LSTM layer\n",
    "    premise_BiLSTM = BiLSTM(premise_translated)\n",
    "    hypothesis_BiLSTM = BiLSTM(hypothesis_translated)\n",
    "    \n",
    "    # Apply Batch normalization\n",
    "    premise_normalized = BatchNormalization()(premise_BiLSTM)\n",
    "    hypothesis_normalized = BatchNormalization()(hypothesis_BiLSTM)\n",
    "\n",
    "    # Concatenate the normalized premise and hypothesis and apply a dropout layer\n",
    "    train_input = concatenate([premise_normalized, hypothesis_normalized])\n",
    "    train_input = Dropout(DROPOUT)(train_input)\n",
    "    \n",
    "    # Apply the (Dense layer, Dropout layer. Batch normalization layer) unit : 1\n",
    "    train_input = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2))(train_input)\n",
    "    train_input = Dropout(DROPOUT)(train_input)\n",
    "    train_input = BatchNormalization()(train_input)\n",
    "\n",
    "    # Apply the (Dense layer, Dropout layer. Batch normalization layer) unit : 2\n",
    "    train_input = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2))(train_input)\n",
    "    train_input = Dropout(DROPOUT)(train_input)\n",
    "    train_input = BatchNormalization()(train_input)\n",
    "\n",
    "    # Apply the (Dense layer, Dropout layer. Batch normalization layer) unit : 3\n",
    "    train_input = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2))(train_input)\n",
    "    train_input = Dropout(DROPOUT)(train_input)\n",
    "    train_input = BatchNormalization()(train_input)\n",
    "\n",
    "    # Define the output Dense layer\n",
    "    prediction = Dense(CATEGORIES, activation='softmax')(train_input)\n",
    "\n",
    "    # Define the complete model\n",
    "    model = Model(inputs=[premise, hypothesis], outputs=prediction)\n",
    "\n",
    "    # Choosing an optimizer\n",
    "    optimizer = RMSprop(lr=LEARNING_RATE, rho=RHO, epsilon=EPSILON, decay=DECAY)\n",
    "    \n",
    "    # Compile the model and print out the model summary\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Training model\")\n",
    "\n",
    "    # ReduceLROnPlateau callback to reduce learning rate when the validation accuracy plateaus\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                                patience=PATIENCE,\n",
    "                                                verbose=1,\n",
    "                                                factor=0.5,\n",
    "                                                min_lr=0.00001)\n",
    "\n",
    "    # Early stopping callback to stop training if we are not making any positive progress\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=PATIENCE)\n",
    "\n",
    "    # ModelCheckpoint callback to save the model with best performance\n",
    "    # A temporary file is created to which the intermediate model weights are stored\n",
    "    _, tmpfn = tempfile.mkstemp()\n",
    "    model_checkpoint = ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    callbacks = [early_stopping, model_checkpoint, learning_rate_reduction]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x=[np.array(premise_list), np.array(hypothesis_list)], y=labels_list, batch_size=BATCH_SIZE, epochs=TRAINING_EPOCHS, validation_split=VALIDATION_SPLIT, callbacks=callbacks)\n",
    "\n",
    "    # Restore the best found model during validation\n",
    "    model.load_weights(tmpfn)\n",
    "\n",
    "   \n",
    "    # Uncomment for generating plots.\n",
    "#     plot(history, \"BiLSTM\")\n",
    "\n",
    "    # Save the model as h5 file\n",
    "    model.save(\"./model/BiLSTM.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f4db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train the model\n",
    "# BiLSTM(training_data[0], training_data[1], training_data[2], embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4e8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfilepath='./Data/xnli_hi_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd4caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_dataset(testfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f88959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...</td>\n",
       "      <td>मैंने फिर उससे बात नहीं की।</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...</td>\n",
       "      <td>मैं इतना परेशान था कि मैंने उससे फिर बात करना ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...</td>\n",
       "      <td>हमारी बहुत अच्छी बातचीत हुई।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>और मैंने सोचा कि यह एक विशेषाधिकार था, और यह अ...</td>\n",
       "      <td>मुझे नहीं पता था कि मैं अकेला ऐसा व्यक्ति नहीं...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>और मैंने सोचा कि यह एक विशेषाधिकार था, और यह अ...</td>\n",
       "      <td>\"उन्होंने कहा,\" \"मुझे लगा था कि मैं एकमात्र ऐस...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...   \n",
       "1  खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...   \n",
       "2  खैर, मैं उस बारे में सोच भी नहीं रहा था, लेकिन...   \n",
       "3  और मैंने सोचा कि यह एक विशेषाधिकार था, और यह अ...   \n",
       "4  और मैंने सोचा कि यह एक विशेषाधिकार था, और यह अ...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0                        मैंने फिर उससे बात नहीं की।      2  \n",
       "1  मैं इतना परेशान था कि मैंने उससे फिर बात करना ...      0  \n",
       "2                       हमारी बहुत अच्छी बातचीत हुई।      1  \n",
       "3  मुझे नहीं पता था कि मैं अकेला ऐसा व्यक्ति नहीं...      1  \n",
       "4  \"उन्होंने कहा,\" \"मुझे लगा था कि मैं एकमात्र ऐस...      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe02133",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_premise_list, test_hypothesis_list, test_labels_list, test_corpus = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6bb2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tokenizer.pickle', \"rb\") as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "# Process the data to integer sequences and labels to one-hot labels\n",
    "sequence = lambda sentence: pad_sequences(tokenizer.texts_to_sequences(sentence), maxlen=MAX_SEQ_LEN)\n",
    "process = lambda item: (sequence(item[0]), sequence(item[1]), to_categorical(item[2]))\n",
    "\n",
    "test_data = process([test_premise_list, test_hypothesis_list, test_labels_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32db7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('./results/BiLSTM.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b33089e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 08:40:15.825299: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('./model/BiLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a85da2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 15s 1s/step - loss: 0.8163 - accuracy: 0.6409\n",
      "Test Loss: 0.82, Test Accuracy: 64.09%\n",
      "\n",
      "157/157 [==============================] - 15s 94ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x=[test_data[0], test_data[1]], y=test_data[2], batch_size=BATCH_SIZE)\n",
    "print(\"Test Loss: {:.2f}, Test Accuracy: {:.2f}%\\n\".format(loss, (accuracy*100)))\n",
    "\n",
    "# Obtain the predicted classes\n",
    "Y_pred = model.predict([test_data[0], test_data[1]])\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "Y_test = np.argmax(test_data[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30146a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(Y_pred.shape[0]):\n",
    "    if Y_pred[index] == 0:\n",
    "        output_file.write(\"Entailment\\n\")\n",
    "    elif Y_pred[index] == 1:\n",
    "        output_file.write(\"Neutral\\n\")\n",
    "    elif Y_pred[index] == 2:\n",
    "        output_file.write(\"Contradiction\\n\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdaaaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# from utils.plot_confusion_matrix import plot_confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "046d99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_mtx = confusion_matrix(Y_test, Y_pred)\n",
    "# plot_confusion_matrix(confusion_mtx, \"\", classes=range(3))\n",
    "\n",
    "# target_names = [\"Class {}\".format(i) for i in range(CATEGORIES)]\n",
    "# classification_rep = classification_report(Y_test, Y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(pd.DataFrame(classification_rep).iloc[:-1, :].T, annot=True)\n",
    "# plt.savefig('./results/classification_report.png')\n",
    "# # plt.show()\n",
    "# plot_model(model, to_file='./results/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ebc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict(premise, hypothesis):\n",
    "    test_premise_list = [premise]\n",
    "    test_hypothesis_list = [hypothesis]\n",
    "    with open('./tokenizer.pickle', \"rb\") as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "\n",
    "    # Process the data to integer sequences and labels to one-hot labels\n",
    "    sequence = lambda sentence: pad_sequences(tokenizer.texts_to_sequences(sentence), maxlen=MAX_SEQ_LEN)\n",
    "    process = lambda item: (sequence(item[0]), sequence(item[1]))\n",
    "\n",
    "    test_data = process([test_premise_list, test_hypothesis_list])\n",
    "    \n",
    "    model = load_model('./model/BiLSTM.h5')\n",
    "    Y_pred = model.predict([test_data[0], test_data[1]])\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    for index in range(Y_pred.shape[0]):\n",
    "        if Y_pred[index] == 0:\n",
    "            print(\"Entailment\")\n",
    "        elif Y_pred[index] == 1:\n",
    "            print(\"Neutral\")\n",
    "        elif Y_pred[index] == 2:\n",
    "            print(\"Contradiction\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "680421ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 815ms/step\n",
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "predict(\"हम घर जाना चाहते हैं\", \"हम घर नहीं जाना चाहते\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a3a59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 802ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"मुझे क्रिकेट खेलने में मजा आता है\", \"मुझे क्रिकेट खेलना पसंद है\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e0eebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 684ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The brown fox sat in front of the fence, The brown fox is near the fence\"\"\"\n",
    "predict(\"भूरी लोमड़ी बाड़े के सामने बैठ गई\", \"भूरी लोमड़ी बाड़ के पास है\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10205c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 161 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc4c017dc60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The brown fox sat in front of the fence, The brown fox is happy\"\"\"\n",
    "predict(\"भूरी लोमड़ी बाड़े के सामने बैठ गई\", \"भूरी लोमड़ी खुश है\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01289576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 162 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc4c017f7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 676ms/step\n",
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The brown fox sat in front of the fence, The brown fox is far from the fence\"\"\"\n",
    "predict(\"भूरी लोमड़ी बाड़े के सामने बैठ गई\", \"भूरी लोमड़ी बाड़ से बहुत दूर है\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51f25701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# from utils.plot_confusion_matrix import plot_confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dabc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_mtx = confusion_matrix(Y_test, Y_pred)\n",
    "# plot_confusion_matrix(confusion_mtx, \"\", classes=range(3))\n",
    "\n",
    "# target_names = [\"Class {}\".format(i) for i in range(CATEGORIES)]\n",
    "# classification_rep = classification_report(Y_test, Y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(pd.DataFrame(classification_rep).iloc[:-1, :].T, annot=True)\n",
    "# plt.savefig('./results/classification_report.png')\n",
    "# # plt.show()\n",
    "# plot_model(model, to_file='./results/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f7965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Y_test[0:5], Y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5845c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24c84413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65124d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09cd83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 648ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The brown fox sat in front of the fence, The brown fox is far from the fence\"\"\"\n",
    "predict(\"भारत ने 2011 में वर्ल्ड कप जीता\", \"2011 में भारत को वर्ल्ड कप मिला\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6367038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 661ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"मुझे खेलों में दिलचस्पी है\", \"मेरी दिलचस्पी क्रिकेट में है\") #Hypernymy - Hyponymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47c79b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 656ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"बराक ओबामा ने भारत का दौरा किया\",\"बराक ओबामा ने मुंबई का दौरा किया\") #Meronymy for places not being captured --- should have been neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83b7d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 643ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"माइकल डेल ने कंपनी के लिए एक नई रणनीति की घोषणा की वह है डेल के संस्थापक\",\"माइकल डेल डेल के संस्थापक हैं\") #coreference capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05b655cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 777ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"हर कर्मचारी को इनकम टैक्स रिटर्न जरूर फाइल करना चाहिए।\",\"कर्मचारी को इनकम टैक्स रिटर्न फाइल करना चाहिए।\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7094cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 642ms/step\n",
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "predict(\"कुछ तोते बाड़ के ऊपर से उड़ गए\", \"सभी तोते बाड़ के ऊपर से उड़ गए\") #Quantifiers handled (Few, all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16a7d2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 638ms/step\n",
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "predict(\"राम ने इनकार किया कि उसने रोटी खाई\", \"राम ने रोटी खाई\") #Should have been neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6efae7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 665ms/step\n",
      "Entailment\n"
     ]
    }
   ],
   "source": [
    "predict(\"तीन महिलाएं घर में खाना बना रही हैं\", \"पार्क में तीन महिलाएं हैं\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495ae64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensyflow",
   "language": "python",
   "name": "tensyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
